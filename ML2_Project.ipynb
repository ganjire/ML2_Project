{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBvDmwQ7NqVVrJWtCbxBt4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganjire/ML2_Project/blob/main/ML2_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Idea: Automated Disease Detection in Plant Leaves using Convolutional Neural Networks (CNNs)**\n",
        "\n",
        "**Project Goal/Motivation**\n",
        "\n",
        "The goal of this project is to develop an automated system that can identify and classify plant diseases from images of plant leaves. This is crucial for agricultural technology as early detection of diseases can lead to timely intervention, reducing both the spread of disease and economic losses. This project will help in understanding the practical application of CNNs in a real-world problem, exploring various architectures, and utilizing transfer learning for effective model performance on a specialized task."
      ],
      "metadata": {
        "id": "UrhxRBjkBTfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "from os import listdir\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
      ],
      "metadata": {
        "id": "tm7pXbTreJbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Collection and Preparation**"
      ],
      "metadata": {
        "id": "vgOww8OPCCYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download PlantVillage-Dataset from Kaggle\n",
        "directory_root = drive.mount('/content/drive')\n",
        "dataset_root = '/content/drive/My Drive/PlantVillage'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WroCNjjimTGu",
        "outputId": "e54e4a89-5fe8-47f7-e4c6-b20f71d8db3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/My Drive/PlantVillage'  # Path to Folder\n",
        "\n",
        "# Right Path to Main Folder\n",
        "categories = os.listdir(data_path)\n",
        "print(\"Categories (Classes):\", categories)\n",
        "\n",
        "# Optional: Number of Images in Subfolders\n",
        "for category in categories:\n",
        "    category_path = os.path.join(data_path, category)\n",
        "    num_images = len(os.listdir(category_path))\n",
        "    print(f\"{category}: {num_images} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FklPNc0jOmi6",
        "outputId": "82f78562-b866-44c6-fe7e-7da9ee016440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories (Classes): ['Tomato__Target_Spot', 'Potato___Early_blight', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato_healthy', 'Tomato__Tomato_mosaic_virus', 'Tomato_Early_blight', 'Potato___Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato_Bacterial_spot', 'Pepper__bell___Bacterial_spot', 'Tomato_Late_blight', 'Potato___healthy', 'Pepper__bell___healthy']\n",
            "Tomato__Target_Spot: 703 images\n",
            "Potato___Early_blight: 500 images\n",
            "Tomato__Tomato_YellowLeaf__Curl_Virus: 1605 images\n",
            "Tomato_healthy: 799 images\n",
            "Tomato__Tomato_mosaic_virus: 190 images\n",
            "Tomato_Early_blight: 504 images\n",
            "Potato___Late_blight: 500 images\n",
            "Tomato_Leaf_Mold: 481 images\n",
            "Tomato_Septoria_leaf_spot: 886 images\n",
            "Tomato_Spider_mites_Two_spotted_spider_mite: 838 images\n",
            "Tomato_Bacterial_spot: 1069 images\n",
            "Pepper__bell___Bacterial_spot: 499 images\n",
            "Tomato_Late_blight: 955 images\n",
            "Potato___healthy: 76 images\n",
            "Pepper__bell___healthy: 743 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def load_images_and_labels(data_directory):\n",
        "    images = []\n",
        "    labels = []\n",
        "    categories = os.listdir(data_directory)\n",
        "\n",
        "    # Create an instance of LabelEncoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    # Fit the encoder to the categories (this assigns an integer to each category)\n",
        "    label_encoder.fit(categories)\n",
        "\n",
        "    for category in categories:\n",
        "        category_path = os.path.join(data_directory, category)\n",
        "        for image_file in os.listdir(category_path):\n",
        "            image_path = os.path.join(category_path, image_file)\n",
        "            try:\n",
        "                with Image.open(image_path) as img:\n",
        "                    img = img.resize((128, 128))  # Resize the image\n",
        "                    img = np.array(img)\n",
        "                    if img.shape == (128, 128, 3):  # Check if the image has three channels\n",
        "                        images.append(img)\n",
        "                        # Encode the category name into an integer\n",
        "                        labels.append(label_encoder.transform([category])[0])\n",
        "            except Exception as e:\n",
        "                print(f\"Can't load image {image_path}: {e}\")\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Assuming data_path is defined and points to your 'plantvillage' folder\n",
        "data_path = '/content/drive/My Drive/PlantVillage'\n",
        "images, labels = load_images_and_labels(data_path)\n",
        "\n",
        "print(f\"Loaded {len(images)} images.\")\n",
        "print(f\"Loaded {len(labels)} labels.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "4kpmXqK4ZNkX",
        "outputId": "acb68cbf-41bf-43f7-dfe8-5930e7325641",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can't load image /content/drive/My Drive/PlantVillage/Tomato__Tomato_YellowLeaf__Curl_Virus/svn-r6Yb5c: cannot identify image file '/content/drive/My Drive/PlantVillage/Tomato__Tomato_YellowLeaf__Curl_Virus/svn-r6Yb5c'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-13214e7b8711>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Assuming data_path is defined and points to your 'plantvillage' folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/PlantVillage'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {len(images)} images.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-13214e7b8711>\u001b[0m in \u001b[0;36mload_images_and_labels\u001b[0;34m(data_directory)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Resize the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load images and labels fron directory**"
      ],
      "metadata": {
        "id": "K0DgBh3vSkeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def load_images_and_labels(data_directory):\n",
        "    images = []\n",
        "    labels = []\n",
        "    categories = os.listdir(data_directory)\n",
        "\n",
        "    for label, category in enumerate(categories):\n",
        "        category_path = os.path.join(data_directory, category)\n",
        "        for image_file in os.listdir(category_path):\n",
        "            image_path = os.path.join(category_path, image_file)\n",
        "            try:\n",
        "                with Image.open(image_path) as img:\n",
        "                    img = img.resize((128, 128))\n",
        "                    img = np.array(img)\n",
        "                    if img.shape == (128, 128, 3):\n",
        "                        images.append(img)\n",
        "                        labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Can't load image {image_path}: {e}\")\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "images, labels = load_images_and_labels(data_path)\n",
        "print(f\"Loaded Images: {len(images)}\")\n",
        "print(f\"Loaded Images: {len(labels)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-dw_jHSOxMj",
        "outputId": "d9d48b7b-7bae-4100-956a-e5824abfcaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can't load image /content/drive/My Drive/PlantVillage/Tomato__Tomato_YellowLeaf__Curl_Virus/svn-r6Yb5c: cannot identify image file '/content/drive/My Drive/PlantVillage/Tomato__Tomato_YellowLeaf__Curl_Virus/svn-r6Yb5c'\n",
            "Loaded Images: 10347\n",
            "Loaded Images: 10347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "print(\"Unique labels (classes):\", unique_labels)\n",
        "print(\"Number of images per class:\", counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJZ71hg3bRqy",
        "outputId": "18ef20ab-86e1-4cfe-b76f-2c116ecb8e98",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels (classes): [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            "Number of images per class: [ 703  500 1604  799  190  504  500  481  886  838 1069  499  955   76\n",
            "  743]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First Split: Split Training Data (70%) und rest (30%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    images, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "# Second Split: Split from Validation Data (20% of 100% -> 2/3 of 30%) and Test Data (10% of 100% -> 1/3 of 30%)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=1/3, random_state=42, stratify=y_temp)\n"
      ],
      "metadata": {
        "id": "PxGrERfvQJvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_train, y_train, X_valid, y_valid, X_test, y_test are already defined\n",
        "\n",
        "# Create generators for training and validation data\n",
        "#train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
        "#valid_generator = test_datagen.flow(X_valid, y_valid, batch_size=32)\n",
        "#test_generator = test_datagen.flow(X_test, y_test, batch_size=32)\n"
      ],
      "metadata": {
        "id": "IWiw4Tn4Vum2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augumentation**"
      ],
      "metadata": {
        "id": "YcbTFZjttkzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ImageDataGenerator for training with augmentation\n",
        "#train_datagen = ImageDataGenerator(\n",
        "  #  rescale=1./255,  # Normalize the image data to [0, 1]\n",
        "   # rotation_range=40,  # Randomly rotate images in the range (degrees, 0 to 180)\n",
        "  #  width_shift_range=0.2,  # Randomly horizontal shift images\n",
        " #   height_shift_range=0.2,  # Randomly vertical shift images\n",
        "#    shear_range=0.2,  # Randomly apply shearing transformations\n",
        "    #zoom_range=0.2,  # Randomly zoom image\n",
        "   # horizontal_flip=True,  # Randomly flip images horizontally\n",
        "  #  fill_mode='nearest'  # Fill in newly created pixels which can appear after a rotation or a width/height shift\n",
        "#)\n",
        "\n",
        "# Define the ImageDataGenerator for validation and test sets (only rescaling)\n",
        "#test_datagen = ImageDataGenerator(rescale=1./255)\n"
      ],
      "metadata": {
        "id": "0nIMSvpqVszN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of Augumentation**"
      ],
      "metadata": {
        "id": "hcWK3tnStv_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a batch of images\n",
        "def plot_images(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip(images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate a batch of images\n",
        "images, _ = next(train_generator)\n",
        "plot_images(images[:5])  # Plot the first 5 images"
      ],
      "metadata": {
        "id": "EbmTPjSlhwLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Modeling**"
      ],
      "metadata": {
        "id": "N1F9dXKHio1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create 1st Model**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J4MouhoLt8t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model_cnn = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(15, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_cnn.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JN_JRsis6FCQ",
        "outputId": "b6cf0197-1d3d-42b6-ab87-f689d46eebf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_55 (Conv2D)          (None, 126, 126, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_43 (MaxPooli  (None, 63, 63, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_56 (Conv2D)          (None, 61, 61, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_44 (MaxPooli  (None, 30, 30, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_57 (Conv2D)          (None, 28, 28, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_45 (MaxPooli  (None, 14, 14, 128)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 512)               12845568  \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 15)                7695      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12946511 (49.39 MB)\n",
            "Trainable params: 12946511 (49.39 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=15)\n",
        "y_valid_one_hot = to_categorical(y_valid, num_classes=15)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=15)\n"
      ],
      "metadata": {
        "id": "VGehof1-cG4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model_cnn.fit(\n",
        "    X_train, y_train_one_hot,\n",
        "    batch_size=32,  # Bestimmen Sie die Batch-Größe nach Ihren Ressourcen\n",
        "    epochs=10,  # Anzahl der Epochen\n",
        "    validation_data=(X_valid, y_valid_one_hot)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5ZVHWb4cLo6",
        "outputId": "1a67b321-7b52-4a79-d60c-7f4dc76d40b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "227/227 [==============================] - 257s 1s/step - loss: 4.2952 - accuracy: 0.4015 - val_loss: 2.0633 - val_accuracy: 0.4208\n",
            "Epoch 2/10\n",
            "227/227 [==============================] - 252s 1s/step - loss: 1.3192 - accuracy: 0.5938 - val_loss: 1.0542 - val_accuracy: 0.6575\n",
            "Epoch 3/10\n",
            "227/227 [==============================] - 256s 1s/step - loss: 0.9446 - accuracy: 0.7002 - val_loss: 1.1400 - val_accuracy: 0.6483\n",
            "Epoch 4/10\n",
            "227/227 [==============================] - 252s 1s/step - loss: 0.6805 - accuracy: 0.7803 - val_loss: 0.7679 - val_accuracy: 0.7676\n",
            "Epoch 5/10\n",
            "227/227 [==============================] - 248s 1s/step - loss: 0.5042 - accuracy: 0.8357 - val_loss: 0.8962 - val_accuracy: 0.7246\n",
            "Epoch 6/10\n",
            "227/227 [==============================] - 251s 1s/step - loss: 0.3687 - accuracy: 0.8771 - val_loss: 0.8516 - val_accuracy: 0.7565\n",
            "Epoch 7/10\n",
            "227/227 [==============================] - 250s 1s/step - loss: 0.3108 - accuracy: 0.9047 - val_loss: 0.9757 - val_accuracy: 0.7551\n",
            "Epoch 8/10\n",
            "227/227 [==============================] - 249s 1s/step - loss: 0.2853 - accuracy: 0.9080 - val_loss: 1.0567 - val_accuracy: 0.7411\n",
            "Epoch 9/10\n",
            "227/227 [==============================] - 247s 1s/step - loss: 0.2036 - accuracy: 0.9348 - val_loss: 0.9591 - val_accuracy: 0.7527\n",
            "Epoch 10/10\n",
            "227/227 [==============================] - 248s 1s/step - loss: 0.2265 - accuracy: 0.9287 - val_loss: 0.9561 - val_accuracy: 0.7671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn.save('/content/drive/My Drive/model_cnn.h5')"
      ],
      "metadata": {
        "id": "umF-3pAoi5zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2nd Model**\n",
        "\n",
        "A deeper model with differnet filter sizes and more layers."
      ],
      "metadata": {
        "id": "7WdVYkz3u8aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn2 = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(256, 256, 3)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(15, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_cnn2.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_cnn2.summary()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0lUe6EyJu0Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create 3rd Model**\n",
        "\n",
        "Different activation functions and dropout rates."
      ],
      "metadata": {
        "id": "m74MnVKCvS-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn3 = Sequential([\n",
        "    Conv2D(32, (3, 3), input_shape=(256, 256, 3)),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3)),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3)),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(256, (3, 3)),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(15, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_cnn3.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_cnn3.summary()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8DOYrjU7vWwR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}